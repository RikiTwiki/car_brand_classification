{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-15 14:00:46.394031: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-15 14:00:46.431770: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-15 14:00:46.432328: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-15 14:00:47.044324: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB7\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.applications import Xception\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'imgs_zip/imgs/train/'\n",
    "VALIDATION_PATH = 'imgs_zip/imgs/validation/'\n",
    "TEST_PATH = 'imgs_zip/imgs/test/'\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = (324, 324)\n",
    "NUM_CLASSES = len(next(os.walk(DATA_PATH))[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'imgs_zip/imgs/train/'\n",
    "\n",
    "train_dataset = []\n",
    "for image_path in os.path.join(image_path, image_path):\n",
    "    # Check if the image file exists\n",
    "    if os.path.isfile(image_path):\n",
    "        train_dataset.append(tf.keras.preprocessing.image.load_img(image_path))\n",
    "\n",
    "train_labels = np.asarray([image['label'] for image in train_dataset])\n",
    "\n",
    "np.save('train_labels.npy', train_labels)\n",
    "\n",
    "train_labels = np.asarray([image['label'] for image in train_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2497 images belonging to 15 classes.\n",
      "Found 206 images belonging to 15 classes.\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-15 14:01:45.680899: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 214990848 exceeds 10% of free system memory.\n",
      "2023-11-15 14:01:45.735323: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 214990848 exceeds 10% of free system memory.\n",
      "2023-11-15 14:01:45.813490: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 214990848 exceeds 10% of free system memory.\n",
      "2023-11-15 14:01:45.849836: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 214990848 exceeds 10% of free system memory.\n",
      "2023-11-15 14:01:45.881277: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 214990848 exceeds 10% of free system memory.\n",
      "/home/sanarip03/.local/lib/python3.10/site-packages/PIL/Image.py:992: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 628s 8s/step - loss: 2.6787 - accuracy: 0.1065 - val_loss: 2.7238 - val_accuracy: 0.0874 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "79/79 [==============================] - 617s 8s/step - loss: 2.6422 - accuracy: 0.1173 - val_loss: 2.7367 - val_accuracy: 0.0874 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "79/79 [==============================] - 620s 8s/step - loss: 2.6333 - accuracy: 0.1177 - val_loss: 2.7550 - val_accuracy: 0.0874 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "79/79 [==============================] - 616s 8s/step - loss: 2.6267 - accuracy: 0.1286 - val_loss: 2.7486 - val_accuracy: 0.0874 - lr: 2.0000e-04\n",
      "Epoch 5/20\n",
      "79/79 [==============================] - 616s 8s/step - loss: 2.6215 - accuracy: 0.1185 - val_loss: 2.7520 - val_accuracy: 0.0874 - lr: 2.0000e-04\n",
      "Epoch 6/20\n",
      "79/79 [==============================] - 616s 8s/step - loss: 2.6184 - accuracy: 0.1229 - val_loss: 2.7463 - val_accuracy: 0.0874 - lr: 4.0000e-05\n",
      "Epoch 7/20\n",
      "19/79 [======>.......................] - ETA: 7:22 - loss: 2.6163 - accuracy: 0.1168"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/sanarip03/Desktop/бренд_машин/efficientnet_model.ipynb Ячейка 4\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sanarip03/Desktop/%D0%B1%D1%80%D0%B5%D0%BD%D0%B4_%D0%BC%D0%B0%D1%88%D0%B8%D0%BD/efficientnet_model.ipynb#W3sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m validation_datagen \u001b[39m=\u001b[39m ImageDataGenerator(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sanarip03/Desktop/%D0%B1%D1%80%D0%B5%D0%BD%D0%B4_%D0%BC%D0%B0%D1%88%D0%B8%D0%BD/efficientnet_model.ipynb#W3sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m     rescale\u001b[39m=\u001b[39m\u001b[39m1.\u001b[39m\u001b[39m/\u001b[39m\u001b[39m255\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sanarip03/Desktop/%D0%B1%D1%80%D0%B5%D0%BD%D0%B4_%D0%BC%D0%B0%D1%88%D0%B8%D0%BD/efficientnet_model.ipynb#W3sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sanarip03/Desktop/%D0%B1%D1%80%D0%B5%D0%BD%D0%B4_%D0%BC%D0%B0%D1%88%D0%B8%D0%BD/efficientnet_model.ipynb#W3sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/sanarip03/Desktop/%D0%B1%D1%80%D0%B5%D0%BD%D0%B4_%D0%BC%D0%B0%D1%88%D0%B8%D0%BD/efficientnet_model.ipynb#W3sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sanarip03/Desktop/%D0%B1%D1%80%D0%B5%D0%BD%D0%B4_%D0%BC%D0%B0%D1%88%D0%B8%D0%BD/efficientnet_model.ipynb#W3sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m     train_datagen\u001b[39m.\u001b[39;49mflow_from_directory(DATA_PATH, target_size\u001b[39m=\u001b[39;49mIMG_SIZE, batch_size\u001b[39m=\u001b[39;49mBATCH_SIZE, class_mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcategorical\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sanarip03/Desktop/%D0%B1%D1%80%D0%B5%D0%BD%D0%B4_%D0%BC%D0%B0%D1%88%D0%B8%D0%BD/efficientnet_model.ipynb#W3sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sanarip03/Desktop/%D0%B1%D1%80%D0%B5%D0%BD%D0%B4_%D0%BC%D0%B0%D1%88%D0%B8%D0%BD/efficientnet_model.ipynb#W3sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m     class_weight\u001b[39m=\u001b[39;49mclass_weights_dict,  \u001b[39m# Apply class weights here\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sanarip03/Desktop/%D0%B1%D1%80%D0%B5%D0%BD%D0%B4_%D0%BC%D0%B0%D1%88%D0%B8%D0%BD/efficientnet_model.ipynb#W3sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m     validation_data\u001b[39m=\u001b[39;49mvalidation_datagen\u001b[39m.\u001b[39;49mflow_from_directory(VALIDATION_PATH, target_size\u001b[39m=\u001b[39;49mIMG_SIZE, batch_size\u001b[39m=\u001b[39;49mBATCH_SIZE, class_mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcategorical\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sanarip03/Desktop/%D0%B1%D1%80%D0%B5%D0%BD%D0%B4_%D0%BC%D0%B0%D1%88%D0%B8%D0%BD/efficientnet_model.ipynb#W3sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[reduce_lr]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sanarip03/Desktop/%D0%B1%D1%80%D0%B5%D0%BD%D0%B4_%D0%BC%D0%B0%D1%88%D0%B8%D0%BD/efficientnet_model.ipynb#W3sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m )\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/sanarip03/Desktop/%D0%B1%D1%80%D0%B5%D0%BD%D0%B4_%D0%BC%D0%B0%D1%88%D0%B8%D0%BD/efficientnet_model.ipynb#W3sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m \u001b[39m# Evaluate the model on the test set\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/sanarip03/Desktop/%D0%B1%D1%80%D0%B5%D0%BD%D0%B4_%D0%BC%D0%B0%D1%88%D0%B8%D0%BD/efficientnet_model.ipynb#W3sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m test_datagen \u001b[39m=\u001b[39m ImageDataGenerator(\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/sanarip03/Desktop/%D0%B1%D1%80%D0%B5%D0%BD%D0%B4_%D0%BC%D0%B0%D1%88%D0%B8%D0%BD/efficientnet_model.ipynb#W3sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m     rescale\u001b[39m=\u001b[39m\u001b[39m1.\u001b[39m\u001b[39m/\u001b[39m\u001b[39m255\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/sanarip03/Desktop/%D0%B1%D1%80%D0%B5%D0%BD%D0%B4_%D0%BC%D0%B0%D1%88%D0%B8%D0%BD/efficientnet_model.ipynb#W3sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/engine/training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m ):\n\u001b[1;32m   1741\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1742\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1743\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1744\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function(\u001b[39m*\u001b[39;49margs))\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[1;32m    197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[1;32m    198\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[1;32m    199\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[1;32m    200\u001b[0m     )\n\u001b[1;32m    201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\u001b[39mself\u001b[39m, \u001b[39mlist\u001b[39m(args))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m   1458\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1459\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[1;32m   1460\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[1;32m   1461\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m   1462\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m   1463\u001b[0m   )\n\u001b[1;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load the pre-trained model\n",
    "base_model = tf.keras.applications.EfficientNetB7(include_top=False, weights='imagenet')\n",
    "\n",
    "# Freeze the pre-trained model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add a global average pooling layer\n",
    "global_average_pooling = tf.keras.layers.GlobalAveragePooling2D()\n",
    "\n",
    "# Add a dense layer with 1024 units\n",
    "dense_layer1 = tf.keras.layers.Dense(1024, activation='relu')\n",
    "\n",
    "# Add a dropout layer with 0.2 dropout rate\n",
    "dropout_layer1 = tf.keras.layers.Dropout(0.2)\n",
    "\n",
    "# Add a dense layer with 512 units\n",
    "dense_layer2 = tf.keras.layers.Dense(512, activation='relu')\n",
    "\n",
    "# Add a dropout layer with 0.2 dropout rate\n",
    "dropout_layer2 = tf.keras.layers.Dropout(0.2)\n",
    "\n",
    "# Add a dense layer with NUM_CLASSES units\n",
    "output_layer = tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "\n",
    "# Create the model\n",
    "model = tf.keras.models.Sequential([\n",
    "    base_model,\n",
    "    global_average_pooling,\n",
    "    dense_layer1,\n",
    "    dropout_layer1,\n",
    "    dense_layer2,\n",
    "    dropout_layer2,\n",
    "    output_layer\n",
    "])\n",
    "\n",
    "# Convert train_labels to a tensor\n",
    "train_labels_tensor = tf.convert_to_tensor(train_labels)\n",
    "\n",
    "# Get unique labels and their counts\n",
    "unique, _, counts = tf.unique_with_counts(train_labels_tensor)\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = tf.math.reciprocal(tf.cast(counts, dtype=tf.float32))\n",
    "class_weights = class_weights / tf.reduce_max(class_weights)\n",
    "\n",
    "# Ensure class weights are in the right format for training\n",
    "class_weights_dict = {i: weight.numpy() for i, weight in enumerate(class_weights)}\n",
    "\n",
    "# weighted_categorical_crossentropy = tf.keras.losses.CategoricalCrossentropy(from_logits=True, class_weights=class_weights)\n",
    "\n",
    "# Compile the model with the updated class weights\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Create a weighted categorical crossentropy loss function\n",
    "# class_weights = tf.cast(tf.math.reciprocal(tf.math.reduce_mean(np.unique(train_labels, return_counts=True)[1])), dtype=tf.float32)\n",
    "# weighted_categorical_crossentropy = tf.keras.losses.CategoricalCrossentropy(from_logits=True, class_weights=class_weights)\n",
    "\n",
    "# Create a class-balanced sampler\n",
    "# class_weights = class_weights / np.max(class_weights)\n",
    "# class_weights = np.power(class_weights, 1/3)\n",
    "# class_weights = tf.cast(class_weights, dtype=tf.float32)\n",
    "# class_balanced_sampler = tf.keras.utils.ClassWeight(class_weights)\n",
    "\n",
    "# Compile the model\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define a learning rate scheduler\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=2,\n",
    "    min_lr=1e-5\n",
    ")\n",
    "\n",
    "# Create a data augmentation pipeline\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_datagen.flow_from_directory(DATA_PATH, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical'),\n",
    "    epochs=20,\n",
    "    class_weight=class_weights_dict,  # Apply class weights here\n",
    "    validation_data=validation_datagen.flow_from_directory(VALIDATION_PATH, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical'),\n",
    "    callbacks=[reduce_lr]\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(\n",
    "    test_datagen.flow_from_directory(TEST_PATH, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow in /home/sanarip03/.local/lib/python3.10/site-packages (2.13.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/sanarip03/.local/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow) (59.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/sanarip03/.local/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/sanarip03/.local/lib/python3.10/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/sanarip03/.local/lib/python3.10/site-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /home/sanarip03/.local/lib/python3.10/site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/sanarip03/.local/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/sanarip03/.local/lib/python3.10/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/sanarip03/.local/lib/python3.10/site-packages (from tensorflow) (0.32.0)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in /home/sanarip03/.local/lib/python3.10/site-packages (from tensorflow) (2.13.1)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /home/sanarip03/.local/lib/python3.10/site-packages (from tensorflow) (2.13.0)\n",
      "Collecting typing-extensions<4.6.0,>=3.6.6\n",
      "  Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /home/sanarip03/.local/lib/python3.10/site-packages (from tensorflow) (1.23.5)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/sanarip03/.local/lib/python3.10/site-packages (from tensorflow) (1.56.2)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/sanarip03/.local/lib/python3.10/site-packages (from tensorflow) (16.0.6)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/sanarip03/.local/lib/python3.10/site-packages (from tensorflow) (3.9.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/sanarip03/.local/lib/python3.10/site-packages (from tensorflow) (4.23.4)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in /home/sanarip03/.local/lib/python3.10/site-packages (from tensorflow) (23.5.26)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/sanarip03/.local/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/sanarip03/.local/lib/python3.10/site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: packaging in /home/sanarip03/.local/lib/python3.10/site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/sanarip03/.local/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/sanarip03/.local/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (0.7.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /home/sanarip03/.local/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/sanarip03/.local/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.3.7)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/lib/python3/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.25.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/sanarip03/.local/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.4.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/sanarip03/.local/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/sanarip03/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (5.3.0)\n",
      "Requirement already satisfied: urllib3<2.0 in /home/sanarip03/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/sanarip03/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/sanarip03/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/sanarip03/.local/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/sanarip03/.local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow) (2.1.2)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/sanarip03/.local/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (3.2.0)\n",
      "Installing collected packages: typing-extensions\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.8.0\n",
      "    Uninstalling typing_extensions-4.8.0:\n",
      "      Successfully uninstalled typing_extensions-4.8.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "filelock 3.12.3 requires typing-extensions>=4.7.1; python_version < \"3.11\", but you have typing-extensions 4.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed typing-extensions-4.5.0\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('CPU')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
